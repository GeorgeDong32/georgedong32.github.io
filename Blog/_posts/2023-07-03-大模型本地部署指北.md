---
title: 本地部署大模型指北
categories: [AI,LLM]
tags: [AI]     # TAG names should always be lowercase
---
# 本地部署大模型指北

一些本地部署大模型的指引。

## LLaMA

==使用时请遵守LLaMA模型和FaceBook(Meta)的相关证书和要求==

### [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)

#### On Windows CPU

##### 1.获取LLaMA原版模型

原版LLaMA模型需要[去LLaMA项目申请使用](https://github.com/facebookresearch/llama)或参考这个[PR](https://github.com/facebookresearch/llama/pull/73/files)或者到[huggingface Model Hub](https://huggingface.co/models)自行下载。因版权问题本项目无法提供下载链接。

##### 2.获取Chinese-LLaMA-Alpaca LoRA模型

前往[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)仓库下载模型，并按照步骤合并模型，别忘了给ymcui大佬点个Star。

按照上述仓库中的[下载模型](https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)和[合并模型](https://github.com/ymcui/Chinese-LLaMA-Alpaca#%E5%90%88%E5%B9%B6%E6%A8%A1%E5%9E%8B)完成对应操作后，进入本地部署阶段。

请注意核对合并后模型 `.pth` 的SHA256值

```
fbfccc91183169842aac8d093379f0a449b5a26c5ee7a298baf0d556f1499b90
```

##### 3.Windows本地部署

由于Windows终端和Windows下cmake的限制，直接采用**llama.cpp**可能会出现一些问题，更推荐使用[josStorer/llama.cpp-unicode-windows](https://github.com/josStorer/llama.cpp-unicode-windows)中已经编译好的[发行版](https://github.com/josStorer/llama.cpp-unicode-windows/releases/tag/v1.0.0)进行部署。

###### Step1.下载发行版

[点击此处下载发行版](https://github.com/josStorer/llama.cpp-unicode-windows/releases/tag/v1.0.0)，下载 `main.exe` 和 `quantize.exe` 放到你喜欢的文件夹里。

###### Step2.量化模型

[下载模型转换脚本](https://github.com/josStorer/llama.cpp-unicode-windows/blob/master/convert-pth-to-ggml.py)，放到上述文件夹里。同时把`.pth`模型也放到这个文件夹中。

文件夹结构应如下：

```
LLaMa/
	- main.exe
	- quantize.exe
	- convert-pth-to-ggml.py
	- zh-models/
		- 7B/
			- consolidated.00.pth
			- params.json
		- tokenizer.model
```

随后开始转换模型到FP16格式，运行以下终端命令，生成文件路径为`zh-models/7B/ggml-model-f16.bin`。

```powershell
python convert-pth-to-ggml.py zh-models/7B/ 1
```

转换过程中请保证有充足的内存，建议开启16G+的虚拟内存以保障转换顺利进行，否则可能导致系统崩溃。

如果你有32G的内存，可以尝试不用进行Q4量化，直接使用FP16格式的模型，运行效果会好一些。若你只有16G的内存，推荐进行Q4量化。

运行以下终端命令进行量化，生成量化模型文件路径为`zh-models/7B/ggml-model-q4_0.bin`。

```
./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin 2
```

此处也可以将最后一个参数改为`3`，即生成`q4_1`版本的量化权重。`q4_1`权重比`q4_0`大一些，速度慢一些，效果方面会有些许提升，具体可参考[llama.cpp#PPL](https://github.com/ggerganov/llama.cpp#perplexity-measuring-model-quality)。

###### Step3.运行模型

运行`./main`二进制文件，`-m`命令指定4-bit量化模型（也可加载ggml-FP16的模型）。以下是解码参数示例：

```
./main -m zh-models/7B/ggml-model-q4_0.bin --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.3
```

以下是参数解释：

```
-ins 启动类ChatGPT对话交流的运行模式
-f 指定prompt模板，alpaca模型请加载prompts/alpaca.txt
-c 控制上下文的长度，值越大越能参考更长的对话历史（默认：512）
-n 控制回复生成的最大长度（默认：128）
-b 控制batch size（默认：8），可适当增加
-t 控制线程数量（默认：4），可适当增加
--repeat_penalty 控制生成回复中对重复文本的惩罚力度
--temp 温度系数，值越低回复的随机性越小，反之越大
--top_p, top_k 控制解码采样的相关参数
```

到此你已经完成了模型的部署，将来使用时都只需要执行Step3即可，除了你运行所用到的模型外，其他的模型均可删除。
